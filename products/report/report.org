#+BEGIN_SRC elisp :eval :results none :exports results
  (coba-define-org-tex-template)
  (setq custom-tex-template (mapconcat 'identity (list
                                                  org-tex-apa-template
                                                  org-tex-math-template
                                                  org-tex-graphix-template                                                  
                                                  ) "\n"))
(coba-define-org-tex-template)
#+END_SRC

#+LATEX_HEADER: \setlength{\parskip}{\baselineskip}%
#+LATEX_HEADER: \setlength{\parindent}{4pt}

#+LATEX_HEADER: \title{\textbf{Research Internship Report\\
#+LATEX_HEADER:  Bayesian Symbolic Regression}}
#+LATEX_HEADER: \author{David Coba \\ St. no. 12439665} 
#+LATEX_HEADER: \course{Psychological Methods}
#+LATEX_HEADER: \affiliation{Research Master's Psychology \\ University of Amsterdam}
#+LATEX_HEADER: \professor{ \hphantom{1cm} \\ % 
#+LATEX_HEADER: Supervised by: \\% 
#+LATEX_HEADER: Don van den Bergh\\%
#+LATEX_HEADER: Eric-Jan Wagenmakers \\%
#+LATEX_HEADER: \hphantom{1cm} }
#+LATEX_HEADER: \duedate{30 August 2021}

#+LATEX_HEADER: \abstract{
#+LATEX_HEADER: Symbolic regression is a supervised machine learning method that constructs mathematical expressions composing basic operations (e.g. addition, multiplication, trigonometric functions, etc.). Therefore, the models are fully interpretable, unlike other machine learning techniques. This allows symbolic regression to act as a bridge between black-box machine learning models and scientific models that are represented mathematically.
#+LATEX_HEADER:
#+LATEX_HEADER: This project aimed to develop and test a Bayesian symbolic regression algorithm, BayesianSR.jl . We build from the model of \textcite{jin2020bsr} and introduce two significant modifications: a symbolic simplification step and the possibility of running multiple Markov chain Monte Carlo chains in parallel. In a simulation study we compared the original algorithm, our modifications and an evolutionary algorithm. We found that the model with the symbolic simplification step performed worse than the original algorithm, but that the multi-chain sampler was more performant. Moreover, Bayesian algorithms and the evolutionary alternative discovered similar equations, but Bayesian models required a fraction of computational resources. However, all symbolic regression methods substantially overfitted the training dataset. Finally, we discuss some challenges of applying symbolic regression in a Bayesian framework.
#+LATEX_HEADER: }

#+LATEX_HEADER: \keywords{Bayesian inference, symbolic regression, machine learning}

#+LATEX_HEADER: \shorttitle{Bayesian symbolic regression}


\thispagestyle{empty}
\maketitle

* Introduction
# (1200 w)
# Describe prior research, a comprehensible literature review of the research field, converging upon the research questions.
# - Describe the state of affairs, including the theoretical framework, in the current research field based on the existing body of literature.
# - Clarify how the previous research eventuates into the research questions of the current proposal

Symbolic regression is a supervised machine learning technique that attempts to find mathematical expressions that describe relations between variables of a dataset. The mathematical expressions are constructed from a prespecified set of features and operators (e.g. addition, multiplication, trigonometric functions, etc.).
Machine learning methods are powerful for predictive purposes, but they are typically black-box models that are not interpretable because they contain a high number of parameters and interactions (e.g. deep neural networks). However, researchers often desire interpretable models that explain something about the system that is being modeled, instead of just predicting the outcome.
Without understanding the systems we study we cannot predict how they would behave under different circumstances than the ones the models are trained on.
On the other hand, interpretable models can guide us to make informed predictions about how a system would behave in different conditions.
The main advantage of symbolic regression is that the resulting models are explicit and interpretable, since the output are mathematical expressions that do not tend to grow too large.
In recent years there has been a movement of researchers arguing in favor of doing more formal modeling in psychological research (e.g. cites:guest2020compu,robinaugh2020calipers,rooij2020theorybeforetest).
Specially in psychological science, where there is a lack of  established formal models,
symbolic regression is a promising tool to aid in the development of formal models of psychological systems.

Moreover, we can compose symbolic regression with other machine learning models and any scientific model that is specified mathematically.
For example, textcite:dandekar2020scimlcovid use symbolic regression and deep learning to extend an epidemiological model of COVID-19 spread parencite:lin2020covid.
The model represents the spread of the pandemic as a system of differential equations, and captures our scientific knowledge about how contagious diseases spread.
However, they did not have a good mathematical expression for a complex
function that quantified the role of quarantine policies on the spread of the virus. 
With their approach they were able to recover---or discover---a mathematical expression for the role of quarantine policies with a plausible scientific interpretation.
This example illustrates the possibilities that symbolic regression offers to bridge black-box machine learning techniques with scientific modeling of complex systems. 

The space of possible mathematical expressions for any given problem is infinitely large,
and therefore it is not viable to explore it exhaustively. There are
multiple methods that aim to recover mathematical expressions from a
dataset, although they are not always called /symbolic regression/ in
the literature. Some of these methods use sparse regression (e.g. cite:brunton2016sindy), neural
networks (e.g.
cite:sahoo2018learning) or a combination of both (e.g. cite:both2021deepmod).
However, the most common approach to perform symbolic regression is to
do a targeted search using evolutionary algorithms, which work by
mimicking the evolution of a population of candidate expressions.
Examples of this approach are the DEAP library
parencite:fortin2012deap, the widely used proprietary software
/Eureqa/[fn:: https://www.datarobot.com/nutonian/]
parencite:schmidt2009eureqa and the open-source implementation
SymbolicRegression.jl/PySR parencite:pysr.
Evolutionary algorithms represent different mathematical expressions as symbolic trees and generate alternative expressions by mutating previous expressions.
For example, a possible mutation of the symbolic tree depicted in Figure ref:fig:symbolictree could be to replace the /sin/ node with another operator, or replacing the terminal node \(x_1\) with a new branch that grows from a new operator. Every expression is assigned a fitness value, and the expressions with higher fitness from the population are selected to reproduce so that the population can evolve over multiple generations.

#+BEGIN_SRC dot :file symbolictree.pdf :cmdline -Kdot -Tpdf
  graph {
          rankdir=UB;
          "*" -- "+"
          "*" -- sin
          "+" -- 2
          "+" -- x₁
          sin -- x₂
      }
#+END_SRC

#+label: fig:symbolictree
#+ATTR_LATEX: :width 7cm :placement [H]
#+caption: Symbolic tree that represents the expression \((2+x_1) \cdot \sin{x_2}\) .
#+RESULTS:
[[file:symbolictree.pdf]]

A novel approach to perform symbolic regression is the use of Bayesian
methods parencites:guimera2020bayesmachin,jin2020bsr. These models
use Markov chain Monte Carlo (MCMC) samplers to explore the space of
possible mathematical expressions and, similarly to the evolutionary
algorithms, they represent the mathematical expressions as symbolic
trees. In this case, instead of /mutating/ the trees we say that MCMC
chains /jump/ from one tree to another, although the modifications can be equivalent (e.g. replacing an operator or growing a new
branch from where a terminal node used to be). Since Bayesian symbolic regression methods are in early stages of development, it is not clear yet how they perform compared to other symbolic regression models.

# Weak argument
# The main advantage of Bayesian symbolic regression over evolutionary symbolic regression is that we can formally define all steps of the process using probability theory.
# For example, one of the main concerns with symbolic regression is that it can overfit a particular dataset if very complex expressions are not penalized.
# In an evolutionary algorithm we can modify the fitness function to penalize complexity in some arbitrary way, but with a Bayesian algorithm we can specify different prior probabilities for different degrees of complexity.

The current state of the art in expression discovery are models that combine symbolic regression and deep neural networks to encode prior information about the structure of the target mathematical expressions.
textcite:cranmer2020discovsymbol fit neural networks that encode prior scientific knowledge about the structure of the target system, and then they recover mathematical expressions with evolutionary algorithms from the networks.
With a different approach, textcite:udrescu2020aifeynm use a neural network to discover hidden structure that is common in physical formulas (e.g. coherent units, symmetry).
These methods require less data, generalize better to out-of-sample data and have better predictive performance than just the neural networks or evolutionary algorithms on their own.
Bayesian symbolic regression is equivalent in scope with the evolutionary or the sparse regression algorithms, and any of them could be used in combination with neural networks or directly over a dataset.

The goal of this internship was to implement an easy to use Bayesian
symbolic regression program and to compare its performance against
evolutionary methods. We build from the Bayesian symbolic regression
model of textcite:jin2020bsr and introduce two main modifications.
In the next section, we provide a mathematical specification of the model and details about the differences between textcite:jin2020bsr algorithm and our implementation.
Afterwards, we compare the different Bayesian models and an symbolic regression evolutionary algorithm.
Finally, we discuss our results, the limitations of this project and some of the challenges of doing symbolic regression under a Bayesian framework.

# \hfill Word count: 531/1200

* Bayesian symbolic regression algorithm

This section provides a complete mathematical specification of the Bayesian symbolic regression model from textcite:jin2020bsr and the differences between their program[fn:: The materials of their project are available at https://github.com/ying531/MCMC-SymReg .] and ours. We have introduced two main modifications: a symbolic simplification step and the option to run multiple MCMC chains simultaneously. Our implementation, BayesianSR.jl[fn:: The program is available at https://github.com/cobac/BayesianSR.jl .], is available as an easy-to-use and performant package implemented in the
Julia programming language parencite:Julia.

** General model structure
The Bayesian symbolic regression model is defined as linear combination of mathematical expressions represented as symbolic trees: \[
y_i = \beta_0 + \beta_1 \Psi_1(\bm{ x }_i) + \dots + \beta_j \Psi_j(\bm{ x }_i) + \dots + \beta_K \Psi_K(\bm{x}_i) + \varepsilon_i \;\text{.}
\] Each symbolic tree \(\Psi_j\) represents a function of the features of the dataset. The total number of trees is defined by the hyperparameter \(K\). The matrix of observations of the features of the dataset is \(\bm x\) and  \(\bm y\) is the outcome variable vector. The subscript \(i\) indexes particular observations of features and outcome variable.
The coefficients of the linear combination are each
\(\beta_j\) and \(\varepsilon_i\) is the residual for a particular observation. We assume the residuals are normally distributed with mean \(0\) and variance \(\sigma^2\).

Every symbolic tree consists of nodes that can be of two types: either a terminal node representing a feature or an operator node representing an operation. Operator nodes can be either binary (e.g. addition, multiplication) or unary (e.g. \(sin\), \(x^2\)), and any combination of operators is possible. There is a mandatory linear operator node \(lt(x| a, b) = a + bx\) that allows to include real-valued numbers in the symbolic trees. Each tree \(\Psi\) is characterized by its structure \(S\) and by the set of linear operator coefficients of all linear operator nodes \(\forall (a, b) \in \Theta\).

** Prior distributions of the model

We can assign prior probabilities for all operators and features. By default we select from the sets of operators and features uniformly, but it is possible to assign weights to each set.

We also specify a tree structure prior distribution \(p(S)\) that governs how to grow a new symbolic tree. To insert a new node \(\eta\) we select either a new operator or terminal node. The probability of selecting a new operator depends on its depth \(d_\eta\) (for the root node of a tree \(d=1\) ) and two hyperparameters \(\alpha \) and \(\beta\):\[
f(\eta, S) = \alpha \left( 1 + d_\eta \right)^{-\beta}  \text{.}
\] Their default values are \(\alpha = 2\) and \(\beta = 1\).
\(p(S)\) is the joint distribution of all nodes of a symbolic tree.

Linear coefficients \(a\) and \(b\) of the linear operator nodes have prior distributions\[
a_\eta \sim N(0, \sigma^2_a) \;\; \text{and} \;\;
b_\eta \sim N(1, \sigma^2_b)
\]
respectively. The prior distribution of the linear coefficients of a symbolic tree \(p(\Theta | S)\) is the joint distribution of all the linear operator coefficients. The variances of the linear coefficients \(\sigma^2_\Theta = \left(\sigma^2_a, \sigma^2_b \right)\) have prior distributions \[
\sigma^2_a \sim IG \left( \nu_a /2, \nu_a \lambda_a /2 \right) 
\;\; \text{and} \;\;
\sigma^2_b \sim IG \left( \nu_b /2, \nu_b \lambda_b /2 \right)
\text{,}
\] where \(IG\) denotes the inverse-gamma distribution and \(\nu_a, \lambda_a,  \nu_b\) and \( \lambda_b\) are prespecified hyperparameters. The default value of all of them is \(1\).

The prior distribution of the variance of the residuals \(p(\sigma^2)\) is \(IG(\nu /2, \nu \lambda /2)\), where \(\nu\) and \(\lambda \) are prespecified hyperparameters. Their default value is \(1\).

** Posterior sampling

For every iteration of the program the MCMC sampler generates a proposal and either accepts or rejects the new sample. Variables with superscript \(X^{(t)}\) denote that they belong to the last sample of the MCMC chain and variables with superscript \(X^{*}\) denote that they are from the proposal. At every iteration the sampler only attempts to update one symbolic tree, and it iterates through all symbolic trees in order.

*** Possible movements between trees

First, the MCMC sampler proposes a new tree structure \(S^{*}_j\) by modifying the symbolic tree of the last accepted sample \(S^{(t)}_j\).
There are 6 possible movements between trees, and at every iteration the algorithm only attempts a single movement. 
  
 - Stay :: There is a \(p_0 = \frac{N_l}{4(N_l + 3)}\) probability that the structure of the tree remains unchanged, where \(N_l\) is the number of linear operator nodes of the tree.

- Grow :: There is a \(p_g = \frac{1- p_0}{3} \cdot \min \left\{ 1, \frac{8}{N_{o} + 2} \right\}\) probability that the structure of the tree grows, where \(N_o\) is the number of operator nodes in the tree. To grow a tree we select a random terminal node and grow a new branch from the tree structure prior distribution.

- Prune :: There is a \(p_p = \frac{1-p_0}{3} - p_g\) probability that the tree structure is pruned. To prune a tree we select a random operator node and substitute it for a terminal node selected from the terminal nodes prior distribution.

- Delete :: There is a \(p_d = \frac{1-p_0}{3} \cdot \frac{N_c}{N_c+3}\) probability that a node is deleted, where \(N_c\) is the number of nodes that are eligible candidates for deletion. Nodes can be deleted if they are operator nodes, and if they are the root node they need to have at least one operator node child. If the selected node is unary we replace it for its child. If its binary and non-root we replace it for one of its children at random. If its binary and root, we replace it for one of its operator children at random.
 
- Insert :: There is a \(p_i = \frac{1-p_0}{3}-p_d\) probability of inserting a new operator node drawn from the operator nodes prior distribution between a random node and its parent. If we insert a binary operator node, the original node becomes its first child and a new child is generated from the tree structure prior distribution.

- Reassign operator :: There is a \(p_{ro} = \frac{1- p_0}{6}\) probability of reassigning a random operator node with a new operator drawn from the operator nodes prior distribution. If we transition from an unary operator to a binary operator, the old child becomes its first child and a new child is generated from the tree structure prior distribution. If we transition from a binary operator to a unary operator, we keep the first child.

- Reassign feature :: There is a \(p_{rf} = \frac{1- p_0}{6}\) probability of reassigning a random terminal node with a new terminal node drawn from the terminal nodes prior distribution.

After each movement we can calculate \(q(S^{*} | S^{(t)})\), the probability of moving from the old tree structure \(S^{(t)}_j\) to the new tree structure \(S^{*}_j\) as well as the probability of the movement in reverse \(q(S^{(t)} | S^{*})\). /Grow and prune/ and /delete and insert/ movement pairs are complementary, since they allow to reverse the other movement of the pair. To calculate \(q(\cdot)\) we need to take into account the probability of selecting the movement between trees, as well as the probabilities of each intermediary step (e.g. choosing a specific node from the tree structure or prior distributions).

*** Updating the linear operator coefficients
  
After generating a new tree structure the MCMC sampler generates a new set of linear operator node coefficients \(\Theta^{*}\). However, the new tree structure may contain a different number of linear operator nodes than the old symbolic tree. If this is the case, \(\Theta\) will have different dimensions between trees, and we need to use reversible jump Markov chain Monte Carlo [[parencite:green1995rjmcmc][RJMCMC,::]] to update the linear coefficients. Three different situations can arise:

- No change :: If the number of linear operator nodes is the same in \(S^{*}\) and in \(S^{(t)}\) no RJMCMC is required. In the implementation of the algorithm from textcite:jin2020bsr the set of new coefficients \(\Theta^{*}\) was generated by sampling from the prior distributions of the coefficients. However, we have chosen to instead generate proposals centered around the previous values of the coefficients adding a random value drawn from a \(N(0, 1)\) distribution. We also sample new values for the variances of the coefficients of the linear operators \((\sigma^2_\Theta)^{*}\).

- Expansion :: If the number of linear operator nodes is greater in \(S^{*}\) than in \(S^{(t)}\) we need to use RJMCMC. We sample auxiliary variables \(U = (u_\Theta, u_n)\), where \(\text{dim}(u_\Theta) = \text{dim}(\Theta^{(t)})\) and \(\text{dim}(u_n) =  \text{dim}(\Theta^{*}) - \text{dim}(\Theta^{(t)})\). We sample \((\sigma^2_a)^{*}\) and \((\sigma^2_b)^{*}\) from the variances prior distributions, and \(u_\Theta\) and \(u_n\) are independently sampled from \(N\left(1, (\sigma^2_a)^{*}\right)\) and \(N\left(0, (\sigma^2_b)^{*}\right)\) respectively. Next, we define a function \(j_e(\Theta^{(t)}, U)\) that generates the new set of parameters \(\Theta^{*}\) and the auxiliary variables \(U^{*}\):\[
  \Theta^{*} = \left( \frac{\Theta^{(t)} + u_\Theta}{2}, u_n \right)\text{,} \; \; U^{*} = \frac{\Theta^{(t)}- u_\Theta}{2}\text{.}\] 

- Shrinkage :: If there are less linear operator nodes in \(S^{*}\) than in \(S^{(t)}\) we also need to use RJMCMC. We divide the original linear coefficients set \(\Theta^{(t)}\) into \(\Theta_0\) and \(\Theta_d\), where \(\Theta_d\) are the coefficients of nodes that are dropped. We sample \((\sigma^2_a)^{*}\) and \((\sigma^2_b)^{*}\) from the variances prior distributions, and the auxiliary variables \(U\) from \(N\left(0, (\sigma^2_a)^{*}\right)\) and \(N\left(0, (\sigma^2_b)^{*}\right)\) respectively, with \(\text{dim}(U) = \text{dim}(\Theta_0)\).
  Next, we define a function \(j_s(\Theta^{(t)}, U)\) that generates the new set of parameters \(\Theta^{*}\) and the auxiliary variables \(U^{*}\):\[
  \Theta^{*} = \Theta_0 + U\text{,} \; \; U^{*} = \left(\Theta_0- U, \Theta_d\right)\text{.}\]

Note that \(\text{dim}(\Theta^{(t)}) + \text{dim}(U) = \text{dim}(\Theta^{*}) + \text{dim}(U^{*})\) in all cases, which is necessary so that the Jacobian determinant in Expression ref:eq:ratiorjmcmc is computable. 
The probability distribution of the auxiliary variables \(U\) and \(U^{*}\) is \(h(\{U , U^{*}\}| \Theta^{(t)}, S^{(t)}, S^{*})\).
 To calculate \(h(\cdot)\) we need to take into account the probability of sampling the variances of the linear operator coefficients \( \left( \sigma_\Theta^2 \right)^{*}\) and the auxiliary variables.

*** Accepting new samples

The last two steps that the MCMC sampler performs are sampling a new variance \((\sigma^2)^{*}\) from the variance prior distribution and optimizing via ordinary least squares the linear coefficients \(\beta.\)
After generating a new proposal the MCMC sampler has to decide whether to accept it, and add the new sample to the MCMC chain, or reject it, and add a new copy of the previous sample to the MCMC chain. The probability of accepting the proposed sample is \(\min\{1, R\}\), where \(R\) is defined in Expression ref:eq:ratiometropolis for the case when no RJMCMC is required and in Expression ref:eq:ratiorjmcmc for the case with RJMCMC.
This probability of accepting a new sample guarantees that the MCMC chains will asymptotically converge with the posterior distribution of all possible models.
For brevity we denote all the variances of the model as \(\Sigma = (\sigma^2, \sigma^2_\Theta)\), and \(p(\Sigma)\) their joint prior distribution. 
Given a particular sample, \( f\left[ \bm y | \text{OLS}(\bm x, S, \Theta), \Sigma \right] \) is the value of the likelihood function of the model, which corresponds to the joint density of the residuals of all observations.

#+NAME: eq:ratiometropolis
\begin{equation}
R = \frac
{f \left[\bm y | \text{OLS}\left(\bm{x}, S^{*}, \Theta^{*}\right), \Sigma^{*} \right]
p(S^{*}) q(S^{(t)} | S^{*}) p(\Theta^{*} | S^{*}) p(\Sigma^{*}) }
{f \left[\bm y | \text{OLS}\left(\bm{x}, S^{(t)}, \Theta^{(t)}\right), \Sigma^{(t)} \right]
p(S^{(t)}) q(S^{*} | S^{(t)}) p(\Theta^{(t)} | S^{(t)}) p(\Sigma^{(t)})}
\end{equation}

#+NAME: eq:ratiorjmcmc
\begin{equation}
R = \frac
{f \left[\bm y | \text{OLS}\left(\bm{x}, S^{*}, \Theta^{*}\right), \Sigma^{*} \right]
p(S^{*}) q(S^{(t)} | S^{*}) p(\Theta^{*} | S^{*}) p(\Sigma^{*}) h\left(U^{*} | \Theta^{(t)}, S^{*}, S^{(t)}\right) }
{f \left[ \bm y | \text{OLS}\left(\bm{x}, S^{(t)}, \Theta^{(t)}\right), \Sigma^{(t)} \right]
p(S^{(t)}) q(S^{*} | S^{(t)}) p(\Theta^{(t)} | S^{(t)}) p(\Sigma^{(t)}) h\left(U | \Theta^{(t)}, S^{*}, S^{(t)}\right)}
\cdot \left |
\frac{\partial j \left( \Theta^{(t)}, U | S^{(t)}, S^{*} \right) }
{\partial \left(\Theta^{(t)}, U\right)}
\right |
\end{equation}

The Jacobian determinant from Expression ref:eq:ratiorjmcmc is equivalent to \(2^{- \text{dim}(\Theta^{(t)})}\) in the expansion case, and to \(2^{\text{dim}(\Theta^{*})}\) in the shrinkage case.

Expression ref:eq:ratiometropolis differs from its counterpart from textcite:jin2020bsr. In their version they omitted some of the components that we include to calculate \(R\) (i.e. \(p(\Theta | S)\) and \(p(\Sigma)\)). Moreover, textcite:jin2020bsr only add accepted samples to the MCMC chain, while we add a copy of the previous sample if the proposal is rejected. We believe our implementation is correct and it guarantees the detailed balance required for the MCMC sampler to asymptotically converge with the posterior distribution of all possible models.
# We have contacted the original authors[fn::https://github.com/ying531/MCMC-SymReg/issues/2] and they will consider our comments in future versions of their paper.

** Modifications to the original algorithm
We devised two modifications of the original algorithm to attempt to
make the MCMC sampler more efficient exploring the posterior space of
all possible expressions. First, there are multiple symbolic trees
that represent equivalent expressions (e.g. \(x + y - y + z\) and
\(x + z\)). To reduce the space of mathematical expressions to explore
we can simplify every new expression that the MCMC sampler accepts.
Moreover, simplifying the expressions also reduces the computational
complexity of the likelihood function, which can lead to faster
computation of the acceptance probabilities. We used the Julia package
SymbolicUtils.jl parencite:gowda2021highperfor to simplify
symbolically every accepted sample. This step introduces extra
computational overhead, but we anticipated that the benefits would
compensate the computational cost. Second, we can run multiple MCMC
chains in parallel initialized with different random expressions. The MCMC sampler
has the option to generate a new proposal from a different chain at
each iteration, instead of using the last sample of the current chain.
We set the default probability of inter-chain sampling at \(0.05\) .
This allows the algorithm to explore multiple regions of posterior
space simultaneously without noticeable overhead in machines with
multiple CPU cores.


* Procedure 
# (1000 w)
# ** Operationalization
# - Operationalize the research questions in a clear manner into a research design/strategy. 
# - Describe the procedures for conducting the research and collecting the data. 
# - *For methodological and/or simulation projects describe the design of the simulation study.*

First, we compared the performance of the implementation of textcite:jin2020bsr against our implementation.
Second, we tested the modifications we propose (i.e. symbolic simplification step, multi-chain sampling and both combined) against the original algorithm and an evolutionary algorithm.
The evolutionary algorithm we have used is SymbolicRegression.jl parencite:pysr, a fast, parallel, distributed and open-source implementation.

We tested all models with data generated from a standard set of functions (Expression ref:eq:standardf) that have been used to benchmark other symbolic regression algorithms parencites:chen2015generalisation,chen2016improving,jin2020bsr,topchy2001faster. We used the same training and test conditions as originally reported by textcite:jin2020bsr:
data generated without noise from \(U(-3, 3)\) for the training set, and from \(U(-3, 3)\), \(U(-6, 6)\) and \(U(3, 6)\) for three different test sets. For each Bayesian algorithm we ran 50 simulations with datasets of 30 observations for 100,000 MCMC iterations for each function. Each model consisted of a linear combination of \(K = 2\) symbolic trees, and we used the default values of all other hyperparameters. We ran the evolutionary algorithm with 5 populations of 1000 members each for 3000 generations, for every function. For the rest of the options we used their default values. We allocated 5 CPU cores for all algorithms that supported parallel computation.

#+NAME: eq:standardf
\begin{align}
f_1(x_0, x_1) &= 2.5x_0^4-1.3x_0^3+0.5x_1^2-1.7x_1 \nonumber \\ 
f_2(x_0, x_1) &= 8x_0^2 + 8x_1^3-15 \nonumber \\
f_3(x_0, x_1) &= 0.2x_0^3+0.5x_1^3-1.2x_1-0.5x_0 \nonumber \\
f_4(x_0, x_1) &= 1.5 \exp(x_0) + 5 \cos (x_1)\nonumber \\
f_5(x_0, x_1) &= 6.0 \sin (x_0) \cos(x_1) \nonumber \\
f_6(x_0, x_1) &= 1.35x_0x_1 + 5.5 \sin \left[ \left(x_0-1\right)\left(x_1-1\right) \right]
\end{align}

# CANCELED: Complexity of the expressions
To evaluate performance we measured two elements: accuracy of the
models and computational cost. We used
the Root Mean Squared Error (RMSE) to measure accuracy. To measure
computational cost we have to take into account two factors. In first
place, some algorithms can run on multiple CPU cores to speed up
computation, while others cannot. Therefore, we used real time to
measure computational cost instead of CPU time because we wanted to
compare how efficient each algorithm is for a normal use case. If we
had used CPU time we would had measured all parallel algorithms to
perform multiple times worse than non-parallel ones. In second place, we used
a modified MCMC sampler that offloads chains from memory to run the
simulations, which is unnecessary during a normal use case and creates
significant overhead. To circumvent this issue we used a standardized
time unit defined as the average time that a single iteration of the
original Bayesian algorithm takes during normal usage. We calculated the
average time that a single iteration takes for the rest of the
algorithms and calculated proportionally the time they took in our
custom units of time.


** CANCELED Maybe not                                             :noexport:
CLOSED: [2021-08-09 Mon 20:21]
Lastly, we will use data from textcite:wagenmakers2008diffusion
available on the R package rtdists parencite:singmann2020rtdists to
explore if we can recover mathematical expressions with a plausible
scientific interpretation using both Bayesian symbolic regression and
the evolutionary algorithm.
  
* Results

First, we compared our implementation against the algorithm from textcite:jin2020bsr written in Python, which took on average \(725ms\) to generate one sample, while ours took \(336\mu s\). This is a speedup of 2188x.

The first modification to the original algorithm we wanted to test was to simplify symbolically each accepted sample. However, Figure ref:fig:original_simplify shows that the simplification step occasionally causes the sampler to get stuck on bad expressions, as evidenced by the stair-like pattern seen for equations 1, 4 and 5.
We also see for all equations sudden spikes. This occurs because of differences in how floating point errors are propagated between the generated expression and the simplified expression. When a floating point number exceeds their maximum size in bytes, the Julia language represents it with an infinity object. Some expressions that were computable become uncomputable after the simplification step (e.g. if a component becomes \(\sin(\text{Inf})\)).
Only for Equations 2 and 6 the simplification step seems to consistently outperform the original algorithm,
and we conclude that in general the simplification step in its current form is not worth the additional computational overhead.

\begin{figure}[H]
\includesvg[width=15.5cm]{../../scripts/figures/original_simplify.svg}
\centering
\caption{\label{fig:original_simplify} Average RMSE across all simulations for all iterations. We show the original algorithm and the modified algorithm with a symbolic simplification step. We capped RMSE values at 500 before averaging so that infinities could be displayed.}
\end{figure}

The second modification we tested was to use multiple MCMC chains, and to allow with some probability each chain to generate a sample from a different chain. 
Figure ref:fig:original_multichain demonstrates that the modified algorithm clearly outperforms the original one for all equations. In Figure ref:fig:ex_chains we see an example of how the chains from the multi-chain algorithm quickly find and remain exploring more optimal areas of posterior space. Additionally, Figure ref:fig:multichain_simplify shows that the algorithm with multi-chain sampling and symbolic simplification has the same issues that the algorithm with only the simplification step: we observe the stair-like anomalies and the sudden spikes. We conclude that multi-chain sampling outperforms the original algorithm and that, even in environments without access to CPUs with multiple cores, this modification is the preferred algorithm amongst all versions.

However, our Bayesian symbolic regression algorithm is substantially overfitting the data from the training set. In Figure ref:fig:multichain_tests we see that our algorithm cannot even model properly the test set generated from the same distribution as the training data. The distributions of RMSE remain mostly constant or random across training, maybe with an exception for the third equation. It is noteworthy that in some cases the initial random guesses of the algorithm are better models of the test sets than samples further on the MCMC chains. If we compare the results from the multi-chain algorithm with the original algorithm (Figure ref:fig:original_tests) we observe that each of them perform slightly better than the other in some cases, but in both cases the programs significantly overfit the training set. 

\begin{figure}[H]
\includesvg[width=15.5cm]{../../scripts/figures/original_multichain.svg}
\centering
\caption{\label{fig:original_multichain} Average RMSE across all simulations for all iterations. We show the original algorithm and the modified algorithm with multi-chain sampling. We capped RMSE values at 500 before averaging so that infinities could be displayed.}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{image1}
\includesvg[width=\linewidth]{../../scripts/figures/ex_chain.svg}
  \caption{The MCMC chain from the original implementation.}
  \label{fig:ex_chains}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
\includesvg[width=\linewidth]{../../scripts/figures/ex_multichain.svg}
  \caption{The five MCMC chains from the multi-chain sampler.}
  \label{fig:ex_multichain}
\end{subfigure}
\caption{RMSE of the samples from different algorithms. Both are from the first simulation with data generated from the first equation.}
\label{fig:ex_chains}
\end{figure}

\newpage

\begin{figure}[H]
\includesvg[width=15.5cm]{../../scripts/figures/multichain_simplify.svg}
\centering
\caption{\label{fig:multichain_simplify} Average RMSE across all simulations for all iterations. We show the modified algorithms with multi-chain sampling alone and combined with symbolic simplification. We capped RMSE values at 500 before averaging so that infinities could be displayed. The RMSE axis is also zoomed-in to show greater detail.}
\end{figure}

\newpage

\begin{figure}[H]
\includesvg[width=15.5cm]{../../scripts/figures/multichain_tests.svg}
\centering
\caption{\label{fig:multichain_tests} Average RMSE across all simulations for all iterations of the multi-chain algorithm. We show values generated from the training and test sets. We capped RMSE values at 2000 before averaging so that infinities could be displayed.}
\end{figure}

\newpage 

\begin{figure}[H]
\includesvg[width=15.5cm]{../../scripts/figures/original_tests.svg}
\centering
\caption{\label{fig:original_tests} Average RMSE across all simulations for all iterations of the original algorithm. We show values generated from the training and test sets. We capped RMSE values at 2000 before averaging so that infinities could be displayed.}
\end{figure}

\newpage 

Finally, we compare the performance of the evolutionary and
the Bayesian algorithms. However, the output of the evolutionary
algorithm is different from the output of the Bayesian algorithm.
Instead of a MCMC chain it reports the Pareto frontier between the
RMSE and the complexity of the expressions. 
The Pareto frontier between two---or more---variables is the set of elements that are more optimal for one of the variables than all other elements with the same values for the other variables.
In this case they are the expressions with the lowest RMSE for all observed complexity values.
Figure
ref:fig:evolutionary shows that the evolutionary algorithm is able to
discover equations that model the data from the training set with similar errors as the Bayesian algorithm. Nevertheless, the evolutionary algorithm is also
overfitting the training data, since the RMSE values for all test sets are greater
than the values corresponding to the lowest complexity. Those simplest
expressions are essentially semi-random guesses, since they are
symbolic trees with a single node representing a real number.
Moreover, this algorithm is designed to run on a different scale than
the Bayesian programs, both in terms of simultaneous parallel processes and
time. We ran the algorithm for the minimum amount of time---while
keeping the number of CPU cores consistent---that produced results similar
to those of the Bayesian algorithm. Whereas a complete run of the
Bayesian algorithm took 33.6s on average the evolutionary algorithm took 364s, a
whole order of magnitude greater.
These differences make it hard to compare both algorithms, but we have shown that under the conditions we have tested, the Bayesian symbolic regression model can discover expressions as good as the evolutionary algorithm in a fraction of the time.

\begin{figure}[H]
\includesvg[width=15.5cm]{../../scripts/figures/evolutionary.svg}
\centering
\caption{\label{fig:evolutionary} Pareto frontiers of RMSE and complexity of the expressions reported by the evolutionary algorithm for each equation. Complexity is an ordinal varible measured in number of nodes of the symbolic trees. Exact values are ommited to avoid gaps and make the visualization clearer.
For equation 2 the RMSE axis is zoomed-in and the missing data points are in the \(10^4\) order of magnitude.
}
\end{figure}
\newpage

** Response time data showcase                                    :noexport:
- Test run of evolutionary vs Bayesian on the dataset
- Interpretability
  
* Discussion

# The first paragraph summarizes the manuscript.
# We have shown that x, y, and z. Moreover, our approach, bla bla bla.

In this report we have evaluated the performance of Bayesian symbolic regression algorithms. We have introduced two significant modifications to the algorithm from textcite:jin2020bsr.
The first one, a symbolic simplification step had issues exploring properly the posterior space. In contrast the second modification, allowing the MCMC sampler to explore multiple areas of posterior space simultaneously, outperformed the original algorithm in all scenarios.
Moreover, our implementation is considerably faster than the implementation from textcite:jin2020bsr, and to our knowledge it is the only Bayesian symbolic regression program available as an user-friendly package.
We have also shown that the Bayesian symbolic regression algorithm finds equally good expressions as the evolutionary program on a fraction of the time. Still, all of the symbolic regression algorithms we have tested substantially overfitted the training data and produced worse models of the test sets than random number generators.
These results are contradictory with what textcite:jin2020bsr report. In their article the expressions that the Bayesian symbolic regression program discovers---under the same conditions---model equally well the training data and the testing data generated from the same distribution.

Our conclusions are necessarily limited to the conditions we have tested. The evolutionary algorithm is conceived to run on a larger timescale than the time we have allowed in our simulations.
It would be appropriate to compare both methods in the future under the recommended conditions of the evolutionary algorithm. Moreover, the issue of overfitting might had been alleviated if we had used bigger sample sizes for each simulation. In this case
we prioritized running our simulations under the same conditions as reported by textcite:jin2020bsr, in order to compare both implementations of the same model.
However, another main limitation is that it is not clear which hyperpameters textcite:jin2020bsr used during their simulations. 
We expect hyperparameters to have an important effect on the outcome of the model. They report that the total number of symbolic trees \(K\) seems to not affect the results, since the algorithm is able to adapt the complexity of each tree to compensate for different values of \(K\). But the hyperparameters that govern how new symbolic trees grow determine how the complexity of the expressions is distributed. Maybe if we had used hyperparameters that favored less complex expressions we would have observed less overfitting, matching what textcite:jin2020bsr report.

The main topic we have not addressed in this report and remains pending for the future is how to select output expressions from the MCMC chains. textcite:jin2020bsr report the last expression from the chains after a prespecified number of iterations. However, we have observed (e.g. in Figure ref:fig:ex_chains a) that the last expression does not have to be a particularly good one. We could choose instead the equation with the highest fit of a chain, but this approach would be biased towards more complex expressions. 
In this case, if the sampler had discovered the correct expression it is guaranteed to have the best fit since the datasets were generated without noise. But during real uses cases, or simulations with noisy data, this approach would worsen the overfitting issues. Alternatively, to restrain overfitting we could report the Pareto frontier between fit and complexity like textcite:pysr do. 
Nevertheless, ideally we would use an approach that allowed us to take advantage of the Bayesian components of the model and generate confidence distributions. For predictive purposes this is easy since we can just generate predictions from all the samples of a chain---possibly after a warm-up period. But it is not clear how to generate confidence distributions if our objective is to discover interpretable expressions. If there is a set of tree structures that the MCMC chains explore frequently, we can use their relative frequencies as confidence distributions. However, we would have to take into consideration the distribution of their linear operator node coefficients. 
One avenue for exploring this approach is to split the sampling algorithm into two separate Gibbs steps. The first step samples a tree structure, whereas the second step samples a new set of linear operators

We are aware that our analysis of the performance of the methods is simplistic, since we have focused mainly on predictive accuracy.
In the future, after developing more mature symbolic regression MCMC samplers, we would have to examine how interpretable are the expressions that the algorithm can discover considering trade-offs between fit, computational resources and interpretability. After all, from our perspective the major advantage of symbolic regression over other methods is its ability to aid in discovering mathematical expressions with plausible scientific interpretations. 
Our preliminary results show that the Bayesian approach is a promising alternative that seems to be able to match the performance of the current most common symbolic regression algorithms.
We believe that Bayesian symbolic regression can become an useful tool to aid in the development of formal scientific models and to researchers that wish to compose machine learning techniques with their scientific models.

* Materials

All project materials are available in the following Open Science Framework repository: 
https://osf.io/p8bg5/ . It includes code for all simulations and analyses, simulations raw output and intermediary analysis results.

\printbibliography

