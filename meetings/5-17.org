#+BEGIN_SRC elisp :eval :results none :exports results
  (coba-define-org-tex-template)
(setq custom-tex-template (mapconcat 'identity (list
                                                org-tex-report-template
                                                org-tex-math-template                                                  
                                                ) "\n\n"))
(coba-define-org-tex-template)
#+END_SRC

* Things to discuss
** Jin's code
- https://github.com/ying531/MCMC-SymReg
** The hacky implementation of LinearCoefs means that the first =k= samples are not proper
** Error with how I was tracking the variances
- Each sample has one residual variance
- But each tree has their own linear coefficients variances
- For now hacky fix
#+BEGIN_SRC julia :eval :session :results silent :exports code
  i > k ? previous_i = i - k : previous_i = i
  old_σ²_a = chain.samples[previous_i].σ²[:σ²_a]
  old_σ²_b = chain.samples[previous_i].σ²[:σ²_b]
#+END_SRC
** Acceptance ratio calculation
*** Error with transition probabilities
- I had MH reversed in my head
- Numerator transition proposal -> old
- Denominator transition old -> proposal
*** Standard MH
- On Jin's paper they don't use the probability of the LinearCoef
  - I understand not using the probability of jumping between coefficients
    - Because it is symmetric (or in this case just from the prior)
- But why not include the probability of each coefficient?
  
f(\Theta | S)

- They don't use either the probability of the variances (residuals + LinearCoef)
  - I had already included it in the ratio calculation without RJMCMC because I hadn't noticed they omit it 

p(\Sigma)

- Maybe I'm just misunderstanding the equations
*** With RJMCM
- I was confused with what operations were set notation or vector notation
- The papers/tutorials you sent me have been the best source
- I thought that some notation in Jin's paper were typos, but I think it makes sense now
  - Except 1 on shrinkage
- I need to refactor parts of the code to accommodate the procedure
- ... And I need a bit of scaffolding with the Jacobian D:
  - O Jin's code they use \(2^\text{no. linear operators}\)
    - I think
**** Expansion
1. \(\Theta\) is the set of old parameters
2. \(\Theta^{*}\) is the set of new parameters
3. We sample variances
4. \(\text{dim}(u_\Theta) = \text{dim}(\Theta)\)
5. \(\text{dim}(u_n) = \text{dim}(\Theta^{*}) - \text{dim}(\Theta)\)
6. We sample \(u_\Theta\) and \(u_n\) from the prior \(p(\Theta)\)
7. \(\Theta^{*} = \left( \text{mean}(\Theta, u_\Theta), u_n \right) \) (set addition)
8. \(h(U) = \prod_{}^{ \forall (u_\Theta \cup u_n)} p(u) \)
9. We keep \(U^{*} = \frac{\Theta - u_\Theta}{2}\)
   - For the jacobian ????
***** Example
\begin{equation*}
\label{}
(\theta_1, \; \theta_2)\to 
(\theta^{*}_1 \dots \theta^{*}_4) 
 = 
j(\Theta, u_\Theta, u_n) = 
(\theta_1 + u_\Theta_1 ,\; \theta_2 + u_\Theta_2, \; u_n_1, \; u_n_2)
\end{equation*}

\begin{equation*}
\label{}
J = \begin{vmatrix} 
\frac{\partial \Theta^{*}}{\partial (\Theta, U)}
\end{vmatrix}
=\begin{vmatrix} 
\frac{\partial j (\Theta, U)}{\partial (\Theta, U)}
\end{vmatrix} =
\end{equation*}


\begin{align*}
\begin{vmatrix}
&\frac{\partial \theta_1 + u_\Theta_1}{\partial \theta_1}
&\frac{\partial \theta_1 + u_\Theta_1}{\partial \theta_2}
&\frac{\partial \theta_1 + u_\Theta_1}{\partial X}
&\frac{\partial \theta_1 + u_\Theta_1}{\partial X} \\
&\frac{\partial \theta_2 + u_\Theta_2}{\partial \theta_1}
&\frac{\partial \theta_2 + u_\Theta_2}{\partial \theta_2}
&\frac{\partial \theta_2 + u_\Theta_2}{\partial X}
&\frac{\partial \theta_2 + u_\Theta_2}{\partial X} \\
&\frac{\partial u_n_1}{\partial \theta_1}
&\frac{\partial u_n_1}{\partial \theta_2}
&\frac{\partial u_n_1}{\partial X}
&\frac{\partial u_n_1}{\partial X} \\
&\frac{\partial u_n_2}{\partial \theta_1}
&\frac{\partial u_n_2}{\partial \theta_2}
&\frac{\partial u_n_2}{\partial X}
&\frac{\partial u_n_2}{\partial X} \\
\end{vmatrix}
\end{align*}

- What are \(X\)?
  - Could be \(u_\Theta\)
  - or \(u_n\)
  - or this doesn't make sense?

**** Shrinkage
1. \(\Theta\) is the set of old parameters
2. We sample variances
3. \(\Theta_0\) are the parameters we keep
4. \(\Theta_d\) are the parameters we discard
5. We sample \(U\) of size \(\text{dim}(\Theta_0)\) from \(N(0, \sigma^2_{a|b})\)
6. \(\Theta^{*} = \Theta_0 + U\) (vector addition)
   - \(\theta^{*}_i = \theta_i + U_i\)
7. \(h(U) = \prod_{}^{\forall U} p(U_i) \)
8. We keep \(U^{*}=(\Theta - U, \Theta_0)\) (set addition)
   - For the jacobian ????

***** Example
\begin{equation*}
\label{}
(\theta_1 \dots \theta_4) \to 
(\theta^{*}_1, \; \theta^{*}_2) = 
j(\Theta, U) = 
(\theta_1 + U_1 ,\; \theta_2 + U_2)
\end{equation*}

\begin{equation*}
\label{}
J = \begin{vmatrix} 
\frac{\partial \Theta^{*}}{\partial (\Theta, U)}
\end{vmatrix}
=\begin{vmatrix} 
\frac{\partial j (\Theta, U)}{\partial (\Theta, U)}
\end{vmatrix} =
\end{equation*}

\begin{align*}
\begin{vmatrix}
&\frac{\partial \theta_1 + U_1}{\partial \theta_1}
&\frac{\partial \theta_1 + U_1}{\partial \theta_2}
&\frac{\partial \theta_1 + U_1}{\partial \theta_3}
&\frac{\partial \theta_1 + U_1}{\partial \theta_4} \\
&\frac{\partial \theta_2 + U_2}{\partial \theta_1}
&\frac{\partial \theta_2 + U_2}{\partial \theta_2}
&\frac{\partial \theta_2 + U_2}{\partial \theta_3}
&\frac{\partial \theta_2 + U_2}{\partial \theta_4} \\
& \cdots &&\\
& \cdots &&\\
\end{vmatrix}
\end{align*}

- What are the  other two rows?
  - The \(U\)s sampled?
    - Shouldn't them be on the denominators as well?
  - Or what happens with the \(U^{*}\)s?
  - Or this doesn't make sense?

\begin{align*}
\begin{vmatrix}
&\frac{\partial \theta_1 + U_1}{\partial \theta_1}
&\frac{\partial \theta_1 + U_1}{\partial \theta_2}
&\frac{\partial \theta_1 + U_1}{\partial \theta_3}
&\frac{\partial \theta_1 + U_1}{\partial \theta_4} \\
&\frac{\partial \theta_2 + U_2}{\partial \theta_1}
&\frac{\partial \theta_2 + U_2}{\partial \theta_2}
&\frac{\partial \theta_2 + U_2}{\partial \theta_3}
&\frac{\partial \theta_2 + U_2}{\partial \theta_4} \\
&\frac{\partial U_1}{\partial \theta_1} 
&\frac{\partial U_1}{\partial \theta_2}
&\frac{\partial U_1}{\partial \theta_3}
&\frac{\partial U_1}{\partial \theta_4}  \\
&\frac{\partial U_2}{\partial \theta_1}
&\frac{\partial U_2}{\partial \theta_2}
&\frac{\partial U_2}{\partial \theta_3}
&\frac{\partial U_2}{\partial \theta_4}  \\
\end{vmatrix}
\end{align*}


** Type stability
#+BEGIN_SRC julia :eval :session :results silent :exports code
  @code_warntype BayesianSR.step(chain, 1, 1, verbose = false)
  # => Body::Union{Float64, BayesianSR.Sample}
#+END_SRC
- Why??
- If I force ~step(...)::Sample~, the warning disappears (as expected)
- No performance differences
- *FIXED:* =NaN= return when ~optimβ!~ failed.
- I believe all the program is type stable now.
** ExprBugs
- There weren't any bugs :D
  - Couldn't reproduce them
  - I misunderstood how mutation, scope, and assigning(=) worked before when I thought there were bugs.
- (apart from the insert! one)
  - https://github.com/sisl/ExprRules.jl/pull/31

* Notes
